# Raft

## 简介

1. 问题分解
   - 领导者选举（leader election）
   - 日志复制（log replication）
   - 安全性（safety）
2. 状态简化

## 复制状态机

**相同的初始状态+相同的输入=相同的结束状态**

多个节点上，从**相同的初始状态**开始，执行相同的一串命令，产生**相同的最终状态**

在Raft中，leader将客户端请求(command)封装到一个个log entry中，将这些log entries复制到所有follower节点，然后大家按相同顺序应用log entries中的command，根据复制状态机的理论，大家的结束状态肯定是一致的。

![image-20241202153340277](Raft%E7%AE%97%E6%B3%95.assets/image-20241202153340277.png)

可以说，我们使用共识算法，就是为了实现复制状态机。一个分布式场景下的各节点间，就是通过共识算法来保证命令序列的一致，从而始终保持它们的**状态一致**，从而实现高可用的。(投票选主是一种特殊的命令)

## 状态简化

在任何时刻，每一个服务器节点都处于leader、follower或candidate这三个状态之一

相比于Paxos，这一点就极大简化了算法的实现，因为Raft只需考虑状态的**切换**，而不用像Paxos那样考虑状态之间的**共存和互相影响**。

![image-20241202153948623](Raft%E7%AE%97%E6%B3%95.assets/image-20241202153948623.png)

Raft把时间分割成任意长度的**任期(term）**，任期用连续的整数标记。
每一段任期从一次选举开始。在某些情况下，一次选举无法选出leader(比如两个节点收到了相同的票数)，在这种情况下，这一任期会以没有leader结束;
一个新的任期(包含一次新的选举)会很快重新开始，Raft保证在任意一个任期内，**最多只有一个leader**。

![image-20241202154229771](Raft%E7%AE%97%E6%B3%95.assets/image-20241202154229771.png)

- Raft算法中服务器节点之间使用**RPC**进行通信，并且Raft中只有两种主要的RPC:  
- **RequestVote RPC(请求投票)**:由candidate在选举期间发起
- **AppendEntriesRPC(追加条目)**:由leader发起，用来复制日志和提供一种心跳机制。
- 服务器之间通信的时候会**交换当前任期号**;如果一个服务器上的当前任期号比其他的小,该服务器会将自己的任期号更新为较大的那个值。
- 如果一个candidate或者leader发现自己的任期号过期了，它会立即回到follower状态
- 如果一个节点接收到一个包含过期的任期号的请求，它会直接拒绝这个请求。

## 领导者选举

Raft内部有一种**心跳机制**，如果存在leader，那么它就会周期性地向所有follower发送心跳，来维持自己的地位。如果follower一段时间没有收到心跳，那么他就会认为系统中没有可用的leader了，然后开始进行选举。
开始一个选举过程后，follower先**增加自己的当前任期号**，并转换到**candidate**状态。然后**投票给自己**，并且并行地向集群中的其他服务器节点发送投票请求(RequestVote RPC)

![image-20241202154951862](Raft%E7%AE%97%E6%B3%95.assets/image-20241202154951862.png)

最终会有三种结果

1. 它获得**超过半数选票**赢得了选举 ->成为主并开始发送心跳

2. 其他节点赢得了选举 ->收到**新leader的心跳**后，如果**新leader的任期号不小于自己当前的任期号**，那么就从candidate回到follower状态。

   （当前candidate的任期号已经比老leader的任期号加一了，所以如果新leader和当前candidate状态均正常，那么他俩的任期号大概率是相等的）

3. 一段时间之后没有任何获胜者 ->每个candidate都在一个自己的**随机选举超时时间**后增加任期号开始新一轮投票

为什么会没有获胜者?比如有多个folower同时成为candidate，得票太过分散，没有任何一个candidate得票超过半数。
论文中给出的**随机**选举超时时间为 **150~300ms**

![image-20241202160056051](Raft%E7%AE%97%E6%B3%95.assets/image-20241202160056051.png)

对于没有成为candidate的follower节点，对于**同一个任期**，会按照**先来先得**的原则投出自己的选票（即同一个任期只有一张选票）。

## 日志复制

Leader被选举出来后，开始为客户端请求提供服务。
客户端怎么知道新leader是哪个节点呢?

客户端随机向一个节点（或者就是老leader）发送请求

1. 该节点正好为leader
2. 这个节点为follower，那么可以通过心跳得知leader的ID，然后可以告知client该找谁
3. 该节点正好宕机了，那么client只能再去找另一个节点，重复此过程

集群中**只要超过半数的节点**仍然可用，那么raft集群就能正常提供服务

Leader接收到客户端的指令后，会把指令作为一个新的条目追加到**日志**中去。
一条日志中需要具有三个信息：

- **状态机指令**
- **leader的任期号**
- **日志号**(日志索引)

![image-20241202161704074](Raft%E7%AE%97%E6%B3%95.assets/image-20241202161704074.png)

生成日志后，Leader**并行**发送**AppendEntriesRPC**给follower，让它们复制该条目。当该条目被**超过半数**的follower复制后，leader就可以在**本地执行该指令**并**把结果返回客户端**
我们把本地执行指令，也就是leader应用日志与状态机这一步，称作**提交**。

问题：**是不是日志复制超过半数就百分百会提交呢？**

答：**不是**，因为follower复制完成，到follower通知leader，再到leader完成提交是需要时间的，这个时间内leader如果宕机了，就无法提交



在此过程中，leader或follower随时都有崩溃或缓慢的可能性，Raft必须要在**有宕机的情况下继续支持日志复制**，并且保证每个副本日志顺序的**一致**(以保证复制状态机的实现)。具体有三种可能：

1. 如果有follower因为某些原因**没有给leader响应**，那么leader会不断地**重发**追加条目请求(**AppendEntries RPC**)，<u>**哪怕leader已经回复了客户端**</u>。
2. 如果有**follower崩溃后恢复**，这时Raft追加条目的**一致性检查**生效，保证follower能按顺序恢复崩溃后的缺失的日志
3. 如果**leader崩溃**，那么崩溃的leader<u>可能已经复制了日志到部分follower但还没有**提交**</u>，而被选出的新leader又可能不具备这些日志，这样就**有部分follower中的日志和新leader的日志不相同**。
   Raft在这种情况下，leader通过**强制follower复制它的日志**来解决不一致的问题，这意味着<u>follower中跟leader冲突的日志条目会被新leader的日志条目覆盖</u>(因为没有提交，所以不违背外部一致性)

![image-20241202163906775](Raft%E7%AE%97%E6%B3%95.assets/image-20241202163906775.png)

Raft的**一致性检查**：leader在每一个发往follower的追加条目RPC中，会放入**前一个日志条目的索引位置和任期号**，如果folower在它的日志中找不到前一个日志，那么它就会拒绝此日志，leader收到follower的拒绝后，会<u>发送前一个日志条目</u>，从而**逐渐向前定位到follower第一个缺失的日志**

![image-20241202163354487](Raft%E7%AE%97%E6%B3%95.assets/image-20241202163354487.png)



- 通过这种机制，leader在当权之后就**不需要任何特殊的操作**来使日志恢复到一致状态
- Leader只需要进行正常的操作，然后日志就能在回复AppendEntries一致性检查失败的时候**自动**趋于一致。
- Leader从来不会覆盖或者删除自己的日志条目。(Append-Only)
- 这样的日志复制机制，就可以保证一致性特性:
  - 只要**过半**的服务器能正常运行，Raft就能够接受、复制并应用新的日志条目（因为可以正常选出leader，并提交日志）
  - 在正常情况下，新的日志条目可以在一个RPC来回中被复制给集群中的过半机器
  - 单个运行慢的follower不会影响整体的性能。（因为只要它慢慢追就行了，其他节点没必要等他）

![image-20241202164308644](Raft%E7%AE%97%E6%B3%95.assets/image-20241202164308644.png)

只有前一个日志的日志号和前一个日志的任期号都与follower中的相同，follower才会认为日志是一致的

RPC Response中的success标志，只有在request的term大于等于自己的term，且request通过了一致性检查后才会返回true

如果leaderCommit > commitlndex,那么把commitIndex设为min(leaderCommit, index of last new entry)

## 安全性

领导者选举和日志复制两个子问题实际上已经涵盖了共识算法的全程，但这两点还不能完全**保证每一个状态机会按照相同的顺序执行相同的命令**。

![image-20241203111510772](Raft%E7%AE%97%E6%B3%95.assets/image-20241203111510772.png)

### 1.Leader宕机处理:选举限制

如果一个follower落后了leader若干条日志(但**没有漏一整个任期**)，那么下次选举中按照领导者选举里的规则，它依旧有可能当选leader。它在当选新leader后就永远也无法补上之前缺失的那部分日志，从而造成状态机之间的不一致。

所以需要对领导者选举增加一个限制，<u>保证被选出来的leader一定包含了之前各任期的**所有被提交的日志条目**</u>。

**RequestVote RPC**执行了这样的限制：<u>RPC中包含了candidate的日志信息，如果投票者自己的日志比candidate的还**新**，它会拒绝掉该投票请求</u>。

![image-20241203112022319](Raft%E7%AE%97%E6%B3%95.assets/image-20241203112022319.png)

Raft通过比较两份日志中最后一条日志条目的索引值和任期号来定义谁的日志比较新。

**如果两份日志最后条目的任期号不同，那么任期号大的日志更“新”**

**如果两份日志最后条目的任期号相同，那么日志较长的那个更“新”。**

![image-20241203112156020](Raft%E7%AE%97%E6%B3%95.assets/image-20241203112156020.png) 

(a) S1是leader

(b) S1宕机，S5通过S3,S4选举成为新leader

(c) S5宕机，S1重启并且选举成功，此时日志2已经被复制到了大多数机器上，**但还没有被提交**

(d) S1宕机，S5通过S2,S3,S4选举成功新leader，S2S3会投票给S5，因为S5任期号更大，并且覆盖follower日志

### 2.Leader宕机处理:新leader是否提交之前任期内的日志条目

一旦当前任期内的某个日志条目已经存储到**过半**的服务器节点上，leader就知道该日志条目可以被**提交**了

![image-20241203113314325](Raft%E7%AE%97%E6%B3%95.assets/image-20241203113314325.png)

follower通过leaderCommit参数可以知道leader提交到了哪个日志，从而判断自己可以提交的日志

follower的**提交触发**：
	下一个AppendEntries RPC：**心跳 or 新日志**

单点提交：leader未等follower直接提交client端（实际上该种策略已经是安全的了，并没有等待集群提交的必要）

集群提交：只有集群中超过半数的节点都完成提交，才任务集群提交完成

解释：

集群有 5个机器 abcde；leader是a，他发送了日志 1 到 b c，此时大多数拥有了该日志，a 进行了commit，然后宕机了，bc没有收到commit消息，但是有日志 1；集群中有4个节点： bc de；再进行选举的话，de不可能成为新的leader（选举会比较日志的新旧程度），新的leader只可能是bc中的一个（假设是 b）。然后他们拥有最新日志，b会开始接受client请求，如果产生了新的日志 2 就会继续复制到 c d e，等日志2被多数节点复制之后，就会提交日志2，此时在他前面的日志1，也会被间接提交，所以不存在这样的情况。



如果某个leader在提交某个日志条目之前崩溃了，以后的leader会试图完成该日志条目的**复制**

复制，而非提交，不能通过心跳提交老日志 

（一般情况下，新leader是具有老leader的日志的，但这些老的日志在新leader中可能还未提交，这时新leader会尝试将这个日志复制给所有其他follower，但不会提交，为什么不能提交呢，因为如下这种情况：）

![image-20241204102107076](Raft%E7%AE%97%E6%B3%95.assets/image-20241204102107076.png)

(c)->(d) S1当选leader，把日志2复制到了大多数结点，但却被日志3覆盖了；而如果S1在(c)中提交了日志2，就会出现不一致了

**<u>Raft永远不会通过计算副本数目的方式来提交之前任期内的日志条目</u>**，只有**当前任期内的日志条目**才通过计算副本数目的方式来提交，因为可以确认自己当前的任期号是最大的

那么老日志如何提交？当前任期的某个日志条目以这种方式被提交，那么由于日志匹配特性**，之前的所有日志条目也都会被间接地提交**。

![image-20241204102550839](Raft%E7%AE%97%E6%B3%95.assets/image-20241204102550839.png)

视频演示

<video src="Raft%E7%AE%97%E6%B3%95.assets/raft scope.mp4"/>

**no-op补丁**

被覆盖的日志2虽然不影响集群的正常性，但是我们不希望看到这样的情况发生，所以出现了no-op补丁

<u>一个节点当选leader后，立刻发送一个自己当前任期的空日志体的AppendEntries RPC。</u>这样，就可以把之前任期内满足提交条件的日志都提交了。

一旦no-op完成复制，就可以把之前任期内符合提交条件的日志保护起来了，从而就可以使它们安全提交。因为没有日志体，这个过程应该是很快的。

目前大部分应用于生产系统的raft算法，都是启用no-op的。

### 3.Follower和Candidate宕机处理

Follower和Candidate崩溃后的处理方式比leader崩溃要简单的多，并且两者的处理方式是相同的。

如果follower或candidate崩溃了，那么后续发送给他们的RequestVote AppendEntries RPCs都会失败。

Raft通过**无限的重试**来处理这种失败，如果崩溃的机器重启了，那么这些RPC就会成功地完成。

如果<u>**一个服务器在完成了一个RPC，但是还没有响应的时候崩溃了，那么它重启之后就会再次收到同样的请求。**</u>**(Raft的RPC都是幂等的)**

### 4.时间与可用性限制


raft算法整体不依赖客观时间，也就是说，<u>哪怕因为网络或其他因素，造成后发的RPC先到，也不会影响raft的正确性</u>。(这点和Spanner不同)

只要整个系统满足下面的时间要求，Raft就可以选举出并维持一个稳定的leader:

**广播时间(broadcastTime)<<选举超时时间(electionTimeout)<<平均故障时间(MTBF)**

广播时间（一次RPC来回的时间）和平均故障时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft的RPC需要接受并将信息落盘，所以广播时间大约是**0.5ms到20ms**，取决于存储的技术。

因此，选举超时时间可能需要在**10ms至500ms**之间。大多数服务器的平均故障间隔时间都在几个月甚至更长。

## 集群成员变更

在需要**改变集群配置**的时候(如<u>增减节点</u>、<u>替换宕机的机器</u>或者<u>改变复制的程度</u>)，Raft可以**进行配置变更自动化**。

自动化配置变更机制最大的难点是**保证轮换过程中不会出现同一个任期的两个leader**，因为转换期间整个集群可能划分为**两个独立的大多数**

下图为三节点（S1,S2,S3）集群扩容到五节点（S1,S2,S3,S4,S5）

方框圈出时段，S1,S2为老配置集群，S3,S4,S5为新配置集群

老配置为三节点，S1,S2可以选出一个leader（2/3）

新配置为五节点，S3,S4,S5可以选出一个leader（3/5）

![image-20241204105257509](Raft%E7%AE%97%E6%B3%95.assets/image-20241204105257509.png)

![img](Raft%E7%AE%97%E6%B3%95.assets/v2-3cfb7761994274da5e73a4eef086b340_r.jpg)

![image-20250228170356157](Raft%E7%AE%97%E6%B3%95.assets/image-20250228170356157.png)

所以配置采用了一种**两阶段**的方法。
集群先切换到一个过渡的配置，称之为**<u>联合一致(joint consensus)</u>**

第一阶段，leader发起$C_{old,new}$，使整个集群进入**联合一致**状态。这时，**所有RPC都要在<u>新旧两个配置中都达到大多数才算成功</u>。**

第二阶段，leader发起$C_{new}$，使整个集群进入新配置状态.这时，所有RPC只要在新配置下能达到大多数就算成功。

![image-20241204144625458](Raft%E7%AE%97%E6%B3%95.assets/image-20241204144625458.png)

一旦某个服务器将该新配置日志条目<u>增加到自己的日志</u>中，他就会用该配置来做出未来所有的决策**(服务器总是使用它日志中最新的配置，无论该配置日志是否已经被提交)**

这意味着leader不用等待$C_{old,new}$和$C_{new}$返回，就会直接使用其中的新规则来做出决策。

我们假设 leader可以在集群成员变更任何时候宕机，大概有以下几种可能：

1. leader在$C_{old,new}$未提交时宕机
2. lead在$C_{old,new}$已提交但$C_{new}$未发起时宕机
3. leader在$C_{new}$已发起时宕机

![image-20241204145535563](Raft%E7%AE%97%E6%B3%95.assets/image-20241204145535563.png)

![image-20241204145805482](Raft%E7%AE%97%E6%B3%95.assets/image-20241204145805482.png)

现在有三个节点，S3是现任leader，<u>增加S4和S5，raft会先将他们设置为只读，等到他们追上日志进度后，才会开始集群成员变更。</u>然后，现任leader S3发起了$C_{old,new}$，并复制给了S4和S5，S3,S4,S5进入联合一致状态。

### 1.leader在$C_{old,new}$未提交时宕机

S1S2超时，开始进行选举，并且可以产生一个老配置的leader.

但是，在联合一致状态下，<u>S3必须要在老配置(S1S2S3)和新配置(S1S2S3S4S5)下都拿到超过半数选票才能当选</u>。

所以S3无法当选leader（因为S1和S2会投给他们自己中的一个节点），集群中<u>只能选出S1S2中的一个leader</u>，S3S4S5中无法选出一个leader

这样集群成员变更就失败了，但不会出现两个leader。



这里还有一种可能，即新选出的leader具有$C_{old,new}$

但宕机了，按照安全性限制，这个新leader无法提交$C_{old,new}$

可以让它继续发送$C_{new}$，继续进行集群成员变更

![image-20241204150754886](Raft%E7%AE%97%E6%B3%95.assets/image-20241204150754886.png)

这时S1当选leader，但这时S1是不可以直接提交$C_{old,new}$的，S1只能继续复制$C_{new}$，他把$C_{new}$复制到了S4S5，构成了新配置集群的大多数，但这时仍不能提交，因为没有S3的反馈，这样提交的$C_{new}$会把$C_{old,new}$也一并提交，这是不安全的

在某些设计中，这里可以强制让$C_{new}$按照联合一致规则提交，如果leader满足不了条件，自动退位。

![image-20241204153514475](Raft%E7%AE%97%E6%B3%95.assets/image-20241204153514475.png)



假设S3没有宕机，并且正常复制$C_{old,new}$，满足了联合一致条件，在这种情况下，leader(S3)的$C_{old,new}$日志在新旧两种配置的集群中都超过半数了，$C_{old,new}$就可以被提交
S2S3/S1S2S3=2/3
S2S3S4/S1S2S3S4S5=3/5

![image-20241204151043431](Raft%E7%AE%97%E6%B3%95.assets/image-20241204151043431.png)

### 2.leader在$C_{old,new}$已提交但$C_{new}$未发起时宕机

这时候选举限制安全性规则决定了选出的新leader一定具有$C_{old,new}$，也就是符合在两种配置集群中都超过半数，已经不存在双leader的可能了，

联合一致状态下，也是可以正常执行命令的，但也<u>需要在两个配置集群中都达到大多数才能提交</u>

$C_{old,new}$提交后，leader就会发起$C_{new}$，这时leader只要满足新配置中的条件，就可以提交日志。
S3S4S5/S1S2S3S4S5=3/5

要$C_{new}$发起，意味着$C_{old,new}$已经被复制到了大多数节点，就不需要再去管老配置。

### 3.leader在$C_{new}$已发起时宕机

已经复制了$C_{new}$的节点会只按新配置选举

没有复制$C_{new}$的节点会按新老配置选举

即有没有复制$C_{new}$的节点都有可能当上leader，但没有复制$C_{new}$的节点选举成功也会发$C_{new}$



**缩减的情况**

由S1S2S3S4S5缩减为S1S2S3，这时如果leader S3宕机了，$C_{new}$会不会被覆盖呢?

不会的，因为处于联合一致状态的节点，也就是只复制了$C_{old,new}$没有复制$C_{new}$的节点，必须要在两个集群都得到大多数选票才能选举成功。

而S2S3不会投票给S1S4S5中的一个，所以S3宕了，只有S2才能当选，已提交的$C_{new}$不会被覆盖。

![image-20241204152742618](Raft%E7%AE%97%E6%B3%95.assets/image-20241204152742618.png)

### 总结

在$C_{old,new}$发起但未提交时，raft集群还未进入联合一致状态。这时leader宕机，可以仅靠老配置选出来的新leader。

一旦$C_{old,new}$提交，raft集群就进入了联合一致状态，这时leader宕机，选出的新leader也要符合联合一致的选票规则了。

$C_{old,new}$提交后，leader就可以发起$C_{new}$，从发起$C_{new}$开始，集群就可以仅靠新配置进行选举和日志复制了。

如果是缩减集群的情况下，leader可能自身就是缩减的对象，那么它会在$C_{new}$复制完成后自动退位。



**补充规则**

1. <u>新增节点时，需要等新增的节点完成日志同步再开始集群成员变更</u>

   这点是防止集群在新增节点还未同步日志时就进入联合一致状态或新配置状态，影响正常命令日志提交。（让新节点在同步完成日志钱不具有投票权，也不参与日志计数，处于只读状态）

2. <u>缩减节点时，leader本身可能就是要缩减的节点，这时它会在**完成$C_{new}$的提交后自动退位**</u>

   在发起$C_{new}$后，要退出集群的leader就会处在操纵一个**不包含它本身**的raft集群的状态下。这时它可以发送$C_{new}$日志，但是日志计数时**不计自身**。

3. 为了避免下线的节点超时选举而影响集群运行，服务器会在它确信集群中有leader存在时拒绝Request Vote RPC

   因为$C_{new}$的新leader不会再发送心跳给要退出的节点，如果这些节点没有及时下线，它们会超时增加任期号后发送Request Vote RPC。虽然它们不可能当选leader，但会导致raft集群进入投票选举阶段，影响集群的正常运行。

   为了解决这个问题，Raft在Request Vote RPC上补充了一个规则：一个节点如果在最小超时时间之内收到了Request Vote RPC，那么它会拒绝此RPC。

   这样，只要follower连续收到leader的心跳，那么退出集群节点的Request Vote RPC就不会影响到raft集群的正常运行了

## 总结与性能测试

### 深入理解复制状态机

共识算法的本质是实现复制状态机。

我们构建分布式存储系统，是为了获取更大的**存储容量(Scalability)**

为了获取更大的存储容量，我们把数据进行**分片(Sharding)**

而更多的机器带来了更高的**出错频率(Fault)**

为了**容错(Fault Tolerance)**，我们要对每个分片建立**副本(Replication)**

而为了维持副本之间的一致，就要引入**共识算法(Consensus)**

而共识算法会需要额外的资源与性能(Low Performance)，这里又会反过来
影响系统的容量和分片数设计。

![image-20241204155518960](Raft%E7%AE%97%E6%B3%95.assets/image-20241204155518960.png)

把复制状态机需要同步的**数据量**按大小进行分类，它们分别适合不同类型
的共识算法。

①数据量非常小，如<u>集群成员信息、配置文件、分布式锁、小容量分布式</u>
<u>任务队列</u>。
->无leader的共识算法(如Basic Paxos)，实现有Chubby，Zookeeper等

②数据量比较大但可以拆分为不相干的各部分，如<u>大规模存储系统</u>。
->有leader的共识算法(如MultiPaxos，Raft)，实现有GFS，HDFS等

③不仅数据量大，数据之间还存在关联

一个共识算法集群容纳不了所有的数据。这种情况下，就要把数据分片(partition)到多个状态机中，状态机之间通过**两阶段提交**来保证一致性

这类场景就主要是一些如Spanner、OceanBase、TiDB等<u>支持分布式事务的分布式数据库</u>。它们通常<u>会对Paxos或Raft等共识算法进行一定的改造，来满足事务级的要求。</u>

![image-20241204160001646](Raft%E7%AE%97%E6%B3%95.assets/image-20241204160001646.png)

### Raft基本概念总结

共识算法的三个主要特性：

1. 共识算法可以保证在任何**非拜占庭情况**下的正确性

   通常来说，共识算法可以解决网络延迟、网络分区、丢包、重复发送、乱序问题，无法解决拜占庭问题(节点发送错误消息，如存储不可靠、消息错误)。

2. 共识算法可以保证在**大多数机器正常**的情况下集群的高可用性，而少部分的机器缓慢不影响整个集群的性能

3. **不依赖外部时间**来保证日志的一致性。

   这一点既是共识算法的优势，因为共识算法不受硬件影响，不会因外部因素造成错误。但也造成了一些限制，让共识算法受网络影响很大，在异地容灾场景下，共识算法的支持性比较差。



raft区分于其他共识算法的三个特征：

**Strongleader**:在Raft中，<u>日志只能从leader流向其他服务器</u>。这简化了复制日志的管理，使得raft更容易理解。

**Leader election**:Raft使用<u>随机计时器进于leader选举</u>。这只需在任何共识算法都需要的心跳(heartbeats)上增加少量机制，同时能够简单快速地解决冲突。

**Membership changes**:Raft使用一种共同一致(joint consensus)的方法来处理集群成员变更的问题，变更时，两种不同的配置的大多数机器会重叠。这允许整个集群在配置变更期间可以持续正常运行。

![image-20241204160616314](Raft%E7%AE%97%E6%B3%95.assets/image-20241204160616314.png)

### 集群成员变更拓展

**联合一致(joint consensus)集群成员变更方法**比较复杂，不太契合raft的易理解性

在Diego Ongaro的博士论文，和后续的大部分对raft实现中，都使用的是另一种更简单的单节点变更方法，即**一次只增减一个节点**，称为**单节点集群成员变更方法**。

每次只增减一个节点，相比于多节点变更，最大的差异是**<u>新旧配置集群的大多数，是一定会有重合的。</u>**

4节点集群中的大多数是3节点，只能容忍一个节点宕机

![image-20241204161252472](Raft%E7%AE%97%E6%B3%95.assets/image-20241204161252472.png)

<u>这个重合的节点，它的这一票只能投给一个节点，要么是新配置，要么是老配置</u>，所以**只增减一个节点是选不出两个leader的**，这样就可以**不经过联合一致，直接从老配置切换到新配置**。如果就是要一次增减多个节点，那么执行多次单节点变更就行

![image-20241205094438365](Raft%E7%AE%97%E6%B3%95.assets/image-20241205094438365.png)

首先对新节点进行日志同步，如何判断新增的节点已经完成了日志同步？

通过分多轮同步的方法来完成同步，论文中给出的参考值是十轮，即可认为新增节点的日志已经足够新，可以开始集群成员变更了

第三步，当前的leader S3直接开始产生并发送$C_{new}$，$C_{new}$只有复制到了三个节点以上才能完成提交



**leader宕机情况讨论**

在$C_{new}$没有复制到大多数节点时leader宕机

选出的新leader可能是S4，(3/4)，具有$C_{new}$，那么它会继续进行集群成员变更。

也可能是S1或S2，(2/3)，没有$C_{new}$，这时集群成员变更失败。

因为S4复制了$C_{new}$，所以它需要三个节点的选票才能当选，也就是S1S2S3至少有两者给它投票了。<u>这也是老配置的大多数</u>，所以这里不会产生多leader现象。

![image-20241205095659973](Raft%E7%AE%97%E6%B3%95.assets/image-20241205095659973.png)

**单节点集群成员变更缺陷**

①**联合一致支持一步完成机器的替换**，比如我们可以通过联合一致的方法把原来集群的(a,b,c)三台机器替换为(d,b,c)三台机器

但使用单节点变更就只能由(a.b,c)替换为(a,b,c,d)再替换为(d,b,c)，需要**两步**

②单节点变更过程必然经历**偶数节点**的状态，这会**降低集群的高可用性**

机器两两分布时，如果发生网络分区，无法选出leader。

针对这种情况的解决办法：优化<u>单节点变更的过程中偶数节点集群的大多数概念</u>，老配置的任意两个节点**(a,b)(a,c)(b,c)也可以算作变更过程中四节点的大多数**，可以让$C_{new}$提交，因为(a,b)(a,c)(b,c)是新老配置的最小交集，所以不会产生多leader

![image-20241205100231455](Raft%E7%AE%97%E6%B3%95.assets/image-20241205100231455.png)

③连续的两次变更，第一步变更的过程中如果出现了切主，那么紧跟着的下一次变更可能出现错误。

![image-20241205100447520](Raft%E7%AE%97%E6%B3%95.assets/image-20241205100447520.png)

图1中，leader S3把$C_{new1}$复制到了S5，到图2中S3宕机了，重新选举，S1依靠S2S4的两票当选leader，这时认为$C_{new1}$变更失败。到了图三中， 新leader S1开始$C_{new2}$的复制，$C_{new2}$向集群中添加S6。

注意：这时的leader S1是不认为集群中有S5的，所以$C_{new2}$还是把集群从四节点变更为五节点，所以S1把$C_{new2}$复制到了S1,S2,S6后，就达到了五节点的大多数，可以提交$C_{new2}$了，认为单节点成员变更完成

![image-20241205101038686](Raft%E7%AE%97%E6%B3%95.assets/image-20241205101038686.png)

到了图4中，S1宕机，S3依靠S3S4S5的选票重新当选leader，但此时S3认为集群是S1到S5五个节点，所以只需要三票即可当选，S3当选leader后就会把$C_{new1}$复制到所有节点，造成已提交的$C_{new2}$被覆盖，因为根本不知道S6的存在，所以S6的不会被覆盖。

解决方法：**新leader必须提交一条自己任期内的no-op日志，才能开始单节点集群成员变更**
这样，图③中，S1在当选新leader后，就可以通过no-op把未提交的$C_{new1}$覆盖掉，再开始$C_{new2}$的复制，就不会出问题

### 日志压缩机制

为什么要进行日志压缩呢，因为随着raft集群的不断运行，各状态机上的log也在不断地累积，总会有一个时间会把状态机的内存打爆，所以我们需要一个机制来安全地清理状态机上的log。

Raft采用的是一种**快照技术**，每个节点在达到一定条件之后，可以把当前日志中的命令都写入自己的快照，然后就可以把已经并入快照的日志都删除了。

快照中一个key只会留有最新的一份value，占用空间比日志小得多

如果一个follower落后leader很多，如果老的日志被清理了，leader怎么同步给follower呢?

Raft的策略是**直接向follower发送自己的快照**

![image-20241205105544160](Raft%E7%AE%97%E6%B3%95.assets/image-20241205105544160.png)

### 只读操作处理

直观上讲，raft的读只要直接读取leader上的结果就行了。

直接从leader的状态机取值，实际上并不是线性一致性读(一般也称作强一致性读)

我们对**线性一致性读**的定义:读到的结果要是**读请求发起时**已经完成提交的结果(快照)

在leader和其他节点发生了网络分区情况下，其他节点可能已经重新选出了一个leader，而如果老leader在没有访问其他节点的情况下直接拿自身的值返回客户端，这个读取的结果就有可能不是最新的。

![image-20241205105904036](Raft%E7%AE%97%E6%B3%95.assets/image-20241205105904036.png)

要追求强一致性读的话，就需要让<u>这个读的过程或结果，也在大多数节点上达到共识</u>

稳妥的方法:把读也当做一个log，由leader发到所有的所有节点上寻求共识，这个读的log提交后，得到的结果是一定符合线性一致性的。（代价太大）

优化后的方法，要符合以下规则

①线性一致性读**一定要发往leader**，如果client发往了follower，那么follower也要转发给leader

②**如果一个leader在它的任期内还没有提交一个日志，那么它要在提交了一个日志后才能反馈client读请求。**(可以通过**no-op**补丁来优化这一点)

因为只有在自己任期内提交了一个日志，leader才能确认之前任期的哪些日志已被提交，才不会出现已提交的数据读取不到的情况。



安全性规则能保证被选出的leader一定具有所有已被提交的日志，但它可能有的日志还没有提交，它并不能确定哪些日志是已提交的，哪些日志没提交，而在它任期内提交一个日志，就能确定这一点

![image-20241205110710818](Raft%E7%AE%97%E6%B3%95.assets/image-20241205110710818.png)

③在进行读操作前，leader要向所有节点**发送心跳，并得到大多数节点的反馈**。(为了确保自己仍是leader）

④leader把自己已提交的日志号设为**readIndex**，只要leader应用到了readIndex的日志，就可以查询状态机结果并返回client了。



优化过后的线性一致性读，也**至少**需要一轮RPC(leader确认的心跳)。并不比写操作快多少(写操作最少也就一轮RPC)

所以，还可以更进一步，因为读的这轮RPC仅仅是为了确认集群中没有新leader产生。那么如果leader上一次心跳发送的时间还不到选举超时时间的下界，集群就不能选出一个新leader，那么这段时间就可以不经过这轮心跳确认，直接返回读的结果。(但不建议使用这种方法）

如果不要求强一致性读， 只要避免脏读，怎么样利用follower承载更大的读压力呢?

①follower接收到读请求后，向leader请求**readIndex**。

②follower等待自身状态机**应用日志到readlndex**。

③follower查询状态机结果，并返回客户端。

## 性能及与Paxos比较

### Raft性能

最根本的，每完成一个日志(命令)的复制与提交，需要的网络(RPC)来回次数。raft在理想情况下，需要一次AppendEntries RPC来回即可提交日志(理论上的极限)

影响Raft性能的因素以及优化方法

①选举及维持leader所需的代价->合理设置选举超时时间。

②**Batch**:一个日志可以包含多个命令，然后批量进行复制，来节省网络

③**Pipeline**: leader不用等待follower的回复，就继续给follower发送下一个日志

![image-20241205111701955](Raft%E7%AE%97%E6%B3%95.assets/image-20241205111701955.png)

原始的raft中，leader会先把日志写入自己的磁盘中，再同步日志给follower，等follower写完磁盘后再回复，这里的两步落盘可以并行进行。

如图所示，leader接收一个日志后，直接发给follower，并同时进行自身的落盘，进一步提高并发

④**Multi-Raft**:将数据分组，每组数据是独立的，用自己的raft来同步

### Raft与Paxos比较

“<u>raft不允许日志空洞</u>，所以性能没Paxos好“

这里的Paxos，实际上指的是一个**能完美处理所有日志空洞带来的边界情况**，并能保证<u>处理这些边界情况的代价</u>，要小于允许日志空洞带来的收益的共识算法。

总结：raft确实有不允许日志空洞这个性能上限，但大部分系统实现，连raft的上限，都是远远没有达到的。所以无需考虑raft本身的瓶颈

raft允许日志空洞的改造 ->ParallelRaft。

## ParallelRaft

ParallelRaft是阿里云原生数据库PolarDB的底层文件PolarFS对Raft的一种优化的实现。
PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Database [VLDB 2018]

![image-20241205112421563](Raft%E7%AE%97%E6%B3%95.assets/image-20241205112421563.png)

实际应用中大多数是<u>并发场景</u>，也就意味着会建立**多个连接**

因为多个连接会并行向follower发送日志的，只要一个连接慢了，那么整个日志的顺序就乱掉了

Follower会拒绝掉没有前序日志的日志，造成大量的失败。

![image-20241205112554992](Raft%E7%AE%97%E6%B3%95.assets/image-20241205112554992.png)

**Multi-Raft**将数据拆成一个个部分(Region)，每个Region单独使用一个Raft组进行同步。这样，不同Region之间的数据就不需要保持顺序了

但Region内部的数据还是要顺序复制与提交，Multi-Raft没有根本解决此问题。

我们把Raft中的限制总结为以下两点：

①**log复制的顺序性**: Raft的follower如果接收了一个日志，意味着它具有这个日志之前的所有日志，并且和leader完全一样。

②**log提交(应用)的顺序性**: Raft的任何节点一旦提交了一个日志，意味着它已经提交了之前的所有日志。

ParallelRaft就要打破这两点规则，让log可以**乱序确认(Out-of-Order Acknowledge)**和**乱序提交(Out-of-Order Commit)**



**乱序确认(Out-of-Order Acknowledge)**：ParallelRaft中，任何log成功持久化后立即返回success，无需等待前序日志复制完成，从而大大降低系统的平均延迟

**乱序提交(Out-of-OrderCommit)**：PrallelRaft中，leader在收到大多数节点的复制成功信息后，无需等待前序日志提交，即可提交当前日志。

当然，直接应用这两个"乱序”会造成算法内错误，所以ParallelRaft采用了一些措施来保证在这两个“乱序”的情况下依旧保持算法的正确性

### 乱序提交

因为有的日志中的命令可能会修改相同的数据，如果跳过空洞先应用了后续的日志，就可能造成状态机间的不一致，导致错误。

—>应该着手让同一内容的日志按顺序应用，而不同内容日志的顺序无所谓，乱序提交就是ok的

为了解决这个问题，ParallelRaft引入了一种名叫**look behind buffer**的数据结构。

ParalelRaft的每个log都附带有一个look behind buffer。 look behind buffer存放了前N个log修改的**LBA(逻辑块地址)**信息。

![image-20241206103946083](Raft%E7%AE%97%E6%B3%95.assets/image-20241206103946083.png)

通过look behind buffer，follower能够知道一个log是否与日志空洞里的log冲突，也就是可以<u>判断出当前log的LBA和其look behind buffer中对应日志空洞的LBA是否重合。</u>

**没有冲突的log entry可以被安全执行**。有冲突的log要加到一个pending list中，等到日志空洞补齐，相关LBA的日志执行完成后，才能执行。

PolarFS把**N设为2**，也就是日志空洞最大为2:

![image-20241206104828548](Raft%E7%AE%97%E6%B3%95.assets/image-20241206104828548.png)

图1 蓝色日志，把x修改为2，通过下方的look behind buffer判断出前面空缺的日志修改的是Y和Z的值，与X不冲突，所以蓝色日志可以提交

图2绿色日志前空缺了三个日志，大于N，不能提交

图3绿色日志把X修改为4，下方的look behind buffer可以看到之前的日志有修改X，所以不能提交

图4前面修改日志X的日志已经提交，那么这时候就可以无视前方的日志空洞直接乱序提交了



### 乱序确认(Out-of-Order Acknowledge)

乱序确认会造成日志存在空洞(hole)，日志空洞会有一个很直接的影响:<u>怎样选举出具有完整日志的leader?</u>

ParallelRaft把选出来的主定义为LeaderCandidate，Leader Candidate只有经过一个Merge阶段弥补完所有日志空洞后才能开始接收并复制日志。

在选举阶段，ParallelRaft的规则与Raft略有差异，会选择具有最新checkpoint的节点当选Leader Candidate.

这时其它没有宕机的节点变为Follower Candidate.(论文中没有具体说明Follower Candidate的组成)



**Merge阶段总体流程**

1)Follower Candidate**发送自己的日志(应该是checkpoint之后的所有日志)**给LeaderCandidate，Leader Candidate把自己的日志与收到的日志进行**Merge**

2Leader Candidate同步**状态信息(推测应该是具体选择哪些log)**给Follower。

3)Leader Candidate提交所有日志并通知Follower Candidate提交。

4)Leader Candidate升级为Leader，并开始自己任期内的服务

![image-20241206110427361](Raft%E7%AE%97%E6%B3%95.assets/image-20241206110427361.png)



进入Merge阶段，Leader Candidate会通过所有Follower Candidate的log来补齐自己所有的日志空洞。

下图中，S2S3宕机，选出S1为LeaderCandidate，S4S5为Follower Candidate。

我们可以把所有的log分为三类

**①已提交的日志**

**在任何节点上已经提交过的日志**，如log index=4的日志，这类日志一定要在Leader Candidate上补上空洞并提交。

对于某些日志，可能**在不同节点上log index相同，term不同，如log index=5的日志**，这类日志要选择term最大的日志。即Leader Candidate要把自己log index=5的日志改为任期号为3的日志，并提交。

![image-20241206110703818](Raft%E7%AE%97%E6%B3%95.assets/image-20241206110703818.png)

**②未提交的日志**

**在任何节点上都未提交过，或确定在大多数节点上都没有的日志**(拥有这个日志的节点数+没有响应(宕机)的节点数<不包含这个日志的节点数)，如logindex=7的日志，这类日志如果Leader Candidate上没有，就可以用空日志替代。

**③不确定提没提交的日志**

**如果拥有这个日志的节点数+没有响应(宕机)的节点数>不包含这个日志的节点数**，如logindex=6的日志，我们就无法判断这个日志是否提交。这时，为了保证安全，Leader Candidate要提交这个日志

![image-20241206111041350](Raft%E7%AE%97%E6%B3%95.assets/image-20241206111041350.png)

**checkpoint**

ParallelRaft的checkpoint就是状态机的一个**快照**。在实际实现中，ParallelRaft会选择有最新checkpoint的节点做Leader Candidate，而不是拥有最新日志的节点，有两个原因：

①Merge阶段时，Leader Candidate可以从其它节点获取最新的日志，并且无需处理checkpoint之前的日志。所以checkpoint越新，Merge价段的代价就越小。
②Leader的checkpoint越新，catch up时就更高效



**catch up**
ParallelRaft把落后的Follower追上Leader的过程称为catch up，有两种类型：

**fast-catch-up**:Folower和Leader差距较小时(差距小于上一个checkpoint)，仅同步日志

**streaming-catch-up**:Follower和Leader差距较大时(差距大于上一个checkpoint)，同步checkpoint和日志

case 1需要streaming-catch-up。
case 2只用fast-catch-up。
case 3中多出来的日志会被Leader覆盖掉，和Raft一样

![image-20241206111414540](Raft%E7%AE%97%E6%B3%95.assets/image-20241206111414540.png)



### 性能

随着IO队列的增大，ParallelRaft的延迟要比Raft低，吞吐量也要显著大于Raft。

![image-20241206111621421](Raft%E7%AE%97%E6%B3%95.assets/image-20241206111621421.png)



在所有**基于leader**的共识算法中，leader最终都需要存储所有已提交的日志。

在一些共识算法中，一个leader<u>可以在被选举出来时没有所有已提交的日志</u>，然后通过**额外的机制**来识别并补充这些日志。

但额外的机制就会造成**更高的复杂度和理解成本**

所以Raft选择一种更简单的方式，来<u>使leader在被选出时天然就具有所有已提交的日志</u>。这样就可以**让日志只能从leader流向Follower，leader永远不会复写自己的日志**

## Q&A

### 1、为什么能保证一个任期内至多只有一个领导者？

可以，通过选举的机制可以保证.

首先，candidate 竞选前会自增 term，因此 term 在总体上为单调递增趋势；

其次，在选举机制上，一个 term 内，一个 follower 只有一票，因此只能投票给一个 candidate；

最后，基于多数派原则，一个 candidate 只有拿到半数以上的赞同票才能当选 leader.

因此，同一个 term 内，不可能出现有两个 candidate 同时获得半数以上的赞同票，因此一个 term 至多只有一个 leader.

### 2、为什么能保证通过任期和索引相同的日志内容一定相同？

首先，预写日志具有 append-only 的性质，只作追加，不存在更新和删除操作；

其次同一个 term 只有一个 leader；

因此，在 term 相同的情况下，所有节点在同一个 index 上的日志都会与 term 内 leader 对应 index 位置的日志保持一致；

综上，term 和 index 共同组成了一个全局唯一标识键. 只要term 和 index 均相同，日志内容一定相同

### 3、如果两个节点中分别存在一笔任期和索引均相同的日志，那么在这两笔日志之前的日志是否也保证在顺序和内容上都完全一致？

可以.

（1）对于节点 a，一笔 term = x、index = y 的预写日志允许被写入的前提是，其上一笔预写日志的任期和索引一定和 term 为 x 的leader 的上一笔日志相同；

（2）由问题2知只要日志的索引和任期相同，其内容一定相同；

（3）进一步采用数学归纳法，可以推导得到，节点 a 从这笔写入日志开始向前追溯的所有日志均与任期为 x 的 leader 完全一致；

（4）倘若节点 b 也存在一笔 term = x、index = y 的日志，那么这笔日志一定也是由 term 为 x 的同一 leader 同步写入；

（5）与（2）（3）同理，节点 b 自此向前的所有日志都会与 leader 完全相同；

（6）于是可得，节点 a、b 自此向前的日志都完全相同.

### 4、关于选举机制方面，如何解决选票瓜分引发的问题？

试想一个场景，集群中有 a、b、c 3 个节点，三者网络环境以及硬件配置都非常相近.

其中，c 为leader，集齐两个节点形成共识即可视为多数派，接下来发生了如下事件：

（1）leader c 某时刻宕机；

（2）节点 a、b 心跳检测计时器同时超时，因此同时发起竞选；

（3）发起竞选时，a、b 都对任期加1，并且都会先把手中的一票投给自己，然后向对方拉票；

（4）由于 a、b 手中的票都已投给自己，因此会分别拒绝对方的拉票请求；

（5）a、b 都无法获得多数派的选票，竞选在同一时刻超时，并且同时发起下一轮竞选；

（6）接下来不断重复（3）-（5）步，a、b 陷入僵持局面，始终无人胜出，导致集群不可用.

解决方案：

每个节点的心跳超时阈值都是一个范围内的随机数，避免多个节点在进入完全相同的竞选节奏. 于是进入 candidate 状态的节点有了先后之分，胜负自然就可见分晓.

### 5、为什么新任 leader 一定拥有旧 leader 已提交的日志？

这是由两阶段提交和选举流程中的多数派原则保证的：

（1）只有被集群多数派完成同步的日志才会被 leader 提交；

（2）在选举流程中，节点只会把票投给日志进度不滞后于自身的 candidate；

（3）在竞选流程，candidate 需要获取多数派的赞同票才能胜任，成为新任 leader.

基于第（3）点可知，新任 leader 的日志进度一定能在竞选流程的多数派中出于不滞后的地位.

而在集群节点个数固定的情况下，本轮竞选流程的多数派和认可前任 leader 同步日志请求的多数派至少存在一个重复的节点，否则就违背了多数派的语义（集群半数以上），因此可以得知，新任 leader 一定拥有前任 leader 那笔被多数派认可的日志，即旧 leader 提交的日志.

### 6、是否一项提议只需要被多数派通过就可以提交？

基于 5 可知，当一笔日志被多数派完成同步后，则后续所有新任的 leader 一定会拥有这笔日志，因此这笔日志已经被集群采纳，可以保证其安全性.

上面的论述看起来天衣无缝，然而，本小节所谈及问题的答案其实是否定的.

下面谈及一个极端的 case，并通过一个机制的补充，来让本节所关心的问题被彻底解决.

![img](Raft%E7%AE%97%E6%B3%95.assets/v2-db3e28758d981ef322671397587bb110_1440w.jpg)

【极端 case】

背景：集群中存在 s1-s5 5 个节点，只需要 3 个节点达成共识，即可形成多数派.

（1）moment1：此时 leader 为 s1，term 为 1，s1 接受了一笔写请求，刚将其同步到 s1、s2，还未形成多数派时，s1 就宕机了；

（2）moment2：s5 收获了 s3、s4、s5 的选票，当选 leader，接受了一笔写请求，只在本机完成预写日志的落盘就宕机了；

（3）moment3：s1 收获了 s1、s2、s3、s4 的选票，重新当选 leader，继续推进 term = 1 时那笔遗留写请求的提交，成功将其同步到了 s1、s2、s3，获得多数派的认同，于是提交这笔写请求. 提交之后，s1 又宕机了.

（4）moment4：s2 由于遗留了一笔 term 为 2 的日志 term，领先集群所有节点，因此可以收获集群所有节点的选票. 于是 s5 再度当选 leader，继续推进 term 为 2 时遗留的写请求，由于这笔日志的 index 与第（3）步中 s1 同步日志的 index 相同，又因为其 term 值更大，最终会覆盖 s1、s2、s3 中的老日志，这就导致一笔已经被 s1 提交的日志最终被 s5 回滚了.

注意，在 raft 算法的设定中，已提交的日志就认为是写入成功了，是绝对不允许被回滚的，这种极端 case 就会 raft 算法的公信力造成破坏.

于是为解决这一问题，raft 算法中新增了一项限制，**新上任的 leader 需要至少完成一笔本任期内的写请求，才能够执行提交动作.**

补充这一设定后，在上述 case 的第（3）步，leader s1 尽管完成了 term 1 遗留日志的同步，也不能执行提交动作. 直到其完成一笔 term 3 的写请求之后，才能执行老日志的提交. 这是因为此时，集群中的多数派已经被同步了 term 3 的日志，即使 s1 再发生宕机情况，s5 也不可能凭借 term2 的遗留日志而重新当选了.

事实上，**在工程实践上，通常每个 leader 上任之后，都会向集群广播同步一笔内容为空的日志，称之为 no-op.** 只要这个请求被提交了，多数派也就写入了一遍当前任期的日志，于是本小节所谈及的异常问题就不可能再发生了.

### 7、leader 向 follower 同步日志时，如何保证不出现乱序、丢失、重复的问题？

不乱序、不重复：follower 同步日志前，会校验上一笔日志是否和 leader 的上一笔完全一致，只有这样才会执行同步动作.

不丢失：基于 ack 机制保证. 倘若 leader 超时未收到 follower 同步日志的 ack，会重发同步日志请求.

### 8、如何保证各节点已提交的预写日志顺序和内容都完全一致？

假设节点 a 最后一笔已提交的预写日志的 term = x、index = y，这说明集群中有多数派认同了 term 为 x 的 leader 同步该笔日志的请求.

首先证明：倘若其他节点在 index = y 位置的日志已提交了，则这笔日志的 term 一定也为 x.

证明思路：倘若节点 b 在 index = y 处的日志已提交，且任期为 z，那么就说明集群中有多数派认可了任期为 z 的 leader 同步的 term = z、index = y 的日志的请求. 由于集群不可能存在两个对立的多数派，因此唯一的可能性就是 z = x，原题得证.

接下来基于 问题2的证明结论，我们可以得知各节点在 term = x、index = y 前面部分的日志也都完全一致，即各节点已提交的预写日志顺序和内容都完全一致.

### 9、如何保证状态机数据的最终一致性？

问题8得知，被提交的预写日志顺序和内容都必然是完全一致的.

又由于只有被提交的预写日志才能被应用到状态机，因此状态机的数据必然会按照正确的顺序和请求内容被依次更新，最终一致性得以保证.

### 10、如何解决网络分区引发的无意义选举问题？

倘若集群产生网络分区，部分处于小分区的节点由于无法接收到 leader 的心跳，导致进入选举流程. 又因为网络分区问题，导致选举始终无法获得多数派的响应，最终 candidate 会无限自增 term. 直到网络恢复的那一刻，由于 candidate 异常的高 term，导致 leader 退位，集群进入新一轮的选举流程.

尽管小分区中的节点由于数据的滞后不可能在选举中胜出，最后必然是大分区中的节点胜任，节点数据的一致性依然可以得到保证. 但是这个无意义的选举过程同样会导致集群陷入暂不可用的阶段. 因此，我们可以通过这样的措施来避免这类无意义的选举：

每个 candidate 发起真实选举之前，会有一个提前试探的过程，试探机制是向集群所有节点发送请求，只有得到多数派的响应，证明自己不存在网络环境问题时，才会将竞选任期自增，并且发起真实的选举流程.

### **11 如何保证客户端提交写请求不丢失、不重复？**

不丢失：通过 ack 机制保证. 客户端超时未收到服务端的 ack，则会重发请求.

不重复：客户端记录写请求的序列号，与服务端交互时透传这个序列号. 最终由服务端的 leader 实现对相同序列号写请求的幂等去重.